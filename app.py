#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å“ç‰Œåˆ†æå™¨ APIåç«¯
"""

import os
import sys
import json
import time
import logging
import threading
from datetime import datetime
from flask import Flask, request, send_file, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
import uuid

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå™¨
from universal_brand_analyzer import UniversalBrandAnalyzer
from convert_csv_to_json_v2 import TikTokDataConverter

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# é…ç½®
UPLOAD_FOLDER = 'uploads'
RESULTS_FOLDER = 'analyzed_data'
ALLOWED_EXTENSIONS = {'json', 'csv'}

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(RESULTS_FOLDER, exist_ok=True)

# å­˜å‚¨åˆ†æä»»åŠ¡çš„å…¨å±€å­—å…¸
analysis_tasks = {}

# ä»»åŠ¡æ¸…ç†é…ç½®
MAX_TASKS_IN_MEMORY = 50  # æœ€å¤§ä¿å­˜çš„ä»»åŠ¡æ•°é‡
TASK_TIMEOUT_HOURS = 24   # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆå°æ—¶ï¼‰

def cleanup_old_tasks():
    """æ¸…ç†æ—§ä»»åŠ¡ä»¥é˜²æ­¢å†…å­˜æ³„æ¼"""
    current_time = datetime.now()
    tasks_to_remove = []
    
    for task_id, task_data in analysis_tasks.items():
        try:
            # æ£€æŸ¥ä»»åŠ¡åˆ›å»ºæ—¶é—´
            created_at_str = task_data.get('created_at')
            if created_at_str:
                created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00').replace('+00:00', ''))
                age_hours = (current_time - created_at).total_seconds() / 3600
                
                # æ¸…ç†è¶…è¿‡24å°æ—¶çš„ä»»åŠ¡
                if age_hours > TASK_TIMEOUT_HOURS:
                    tasks_to_remove.append(task_id)
                    print(f"Marking task {task_id} for cleanup (age: {age_hours:.1f} hours)")
            
        except Exception as e:
            print(f"Error checking task {task_id} age: {e}")
            # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼Œä¹Ÿæ ‡è®°ä¸ºåˆ é™¤
            tasks_to_remove.append(task_id)
    
    # å¦‚æœä»»åŠ¡æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œæ¸…ç†æœ€æ—§çš„å·²å®Œæˆä»»åŠ¡
    if len(analysis_tasks) > MAX_TASKS_IN_MEMORY:
        completed_tasks = [(task_id, task_data) for task_id, task_data in analysis_tasks.items() 
                          if task_data.get('status') in ['completed', 'error']]
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
        completed_tasks.sort(key=lambda x: x[1].get('created_at', ''))
        excess_count = len(analysis_tasks) - MAX_TASKS_IN_MEMORY
        
        for i in range(min(excess_count, len(completed_tasks))):
            task_id = completed_tasks[i][0]
            if task_id not in tasks_to_remove:
                tasks_to_remove.append(task_id)
                print(f"Marking task {task_id} for cleanup (memory limit)")
    
    # æ‰§è¡Œæ¸…ç†
    for task_id in tasks_to_remove:
        try:
            del analysis_tasks[task_id]
            print(f"Cleaned up task {task_id}")
        except KeyError:
            pass
    
    if tasks_to_remove:
        print(f"Cleaned up {len(tasks_to_remove)} old tasks. Remaining: {len(analysis_tasks)}")

def start_cleanup_thread():
    """å¯åŠ¨æ¸…ç†çº¿ç¨‹"""
    def cleanup_worker():
        while True:
            try:
                cleanup_old_tasks()
                time.sleep(3600)  # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
            except Exception as e:
                print(f"Error in cleanup thread: {e}")
                time.sleep(600)  # å‡ºé”™æ—¶ç­‰å¾…10åˆ†é’Ÿå†è¯•
    
    cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
    cleanup_thread.start()
    print("Started task cleanup thread")

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

class WebAppHandler(logging.Handler):
    """è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œç”¨äºå®æ—¶æ˜¾ç¤ºæ—¥å¿—"""
    
    def __init__(self, task_id):
        super().__init__()
        self.task_id = task_id
        self.logs = []
    
    def emit(self, record):
        log_entry = self.format(record)
        self.logs.append({
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'level': record.levelname,
            'message': log_entry
        })
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if self.task_id in analysis_tasks:
            analysis_tasks[self.task_id]['logs'] = self.logs

class LoggingCSVConverter:
    """å¸¦æ—¥å¿—åŠŸèƒ½çš„CSVè½¬æ¢å™¨åŒ…è£…ç±»"""
    
    def __init__(self, task_id, logger):
        self.task_id = task_id
        self.logger = logger
        self.converter = TikTokDataConverter(max_workers=5)
        
    def log(self, message):
        """å‘é€æ—¥å¿—æ¶ˆæ¯"""
        self.logger.info(message)
        # åŒæ—¶æ›´æ–°ä»»åŠ¡è¿›åº¦
        if self.task_id in analysis_tasks:
            analysis_tasks[self.task_id]['progress'] = message
    
    def process_csv_with_logging(self, csv_file_path, output_file_path, max_records=None):
        """å¸¦æ—¥å¿—çš„CSVå¤„ç†è¿‡ç¨‹"""
        import csv
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        self.log("ğŸ” Reading CSV file...")
        
        # è¯»å–CSVæ•°æ®
        video_data_list = []
        try:
            with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.reader(csvfile)
                next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
                
                for row in reader:
                    if len(row) >= 2:
                        video_link = row[0].strip()
                        creator_handler = row[1].strip()
                        
                        if video_link and creator_handler:
                            video_data_list.append((video_link, creator_handler))
                            
                            if max_records and len(video_data_list) >= max_records:
                                break
                                
        except Exception as e:
            self.log(f"âŒ Error reading CSV file: {str(e)}")
            return []
        
        self.log(f"ğŸ“Š Found {len(video_data_list)} records to process")
        self.log(f"ğŸš€ Starting conversion with {self.converter.max_workers} workers...")
        
        # å¤„ç†æ•°æ®
        results = []
        processed_count = 0
        
        with ThreadPoolExecutor(max_workers=self.converter.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_data = {
                executor.submit(self.converter.process_single_video, video_link, creator_handler): (video_link, creator_handler)
                for video_link, creator_handler in video_data_list
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_data):
                video_link, creator_handler = future_to_data[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                        processed_count += 1
                        
                        # æ¯å¤„ç†5ä¸ªcreatoræ›´æ–°ä¸€æ¬¡è¿›åº¦
                        if processed_count % 5 == 0:
                            self.log(f"âš¡ Progress: {processed_count}/{len(video_data_list)} creators processed...")
                            
                except Exception as e:
                    self.log(f"âš ï¸ Failed to process {creator_handler}: {str(e)}")
        
        # ä¿å­˜ç»“æœ
        try:
            import json
            with open(output_file_path, 'w', encoding='utf-8') as jsonfile:
                json.dump(results, jsonfile, indent=2, ensure_ascii=False)
            self.log(f"âœ… Successfully saved {len(results)} records to {output_file_path}")
        except Exception as e:
            self.log(f"âŒ Error saving JSON file: {str(e)}")
            return []
        
        self.log(f"ğŸ‰ CSV conversion completed! {len(results)} valid records converted")
        return results

def run_analysis(task_id, file_path, is_csv=False):
    """åœ¨åå°è¿è¡Œåˆ†æä»»åŠ¡"""
    try:
        # ç¡®ä¿ä»»åŠ¡å­˜åœ¨äºå­—å…¸ä¸­
        if task_id not in analysis_tasks:
            print(f"Warning: Task {task_id} not found in analysis_tasks")
            return
            
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        analysis_tasks[task_id]['status'] = 'running'
        analysis_tasks[task_id]['progress'] = 'Starting analysis...'
        
        print(f"Starting analysis for task {task_id}, file: {file_path}, is_csv: {is_csv}")
        
        # åˆ›å»ºè‡ªå®šä¹‰loggerï¼ˆæ— è®ºæ˜¯å¦CSVéƒ½éœ€è¦ï¼‰
        custom_logger = logging.getLogger(f'analyzer_{task_id}')
        custom_logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        custom_logger.handlers = []
        
        # è®¾ç½®è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨
        handler = WebAppHandler(task_id)
        handler.setFormatter(logging.Formatter('%(message)s'))
        custom_logger.addHandler(handler)
        
        # å¦‚æœæ˜¯CSVæ–‡ä»¶ï¼Œå…ˆè¿›è¡Œè½¬æ¢
        if is_csv:
            custom_logger.info('ğŸ“‚ CSV file detected, starting conversion...')
            analysis_tasks[task_id]['progress'] = 'Converting CSV to JSON...'
            
            # åˆ›å»ºå¸¦æ—¥å¿—çš„è½¬æ¢å™¨
            logging_converter = LoggingCSVConverter(task_id, custom_logger)
            
            # ç”Ÿæˆä¸´æ—¶JSONæ–‡ä»¶è·¯å¾„
            json_file_path = file_path.replace('.csv', '_converted.json')
            
            # æ‰§è¡Œè½¬æ¢
            results = logging_converter.process_csv_with_logging(file_path, json_file_path, max_records=None)
            
            if not results:
                analysis_tasks[task_id]['status'] = 'error'
                analysis_tasks[task_id]['progress'] = 'CSVè½¬æ¢å¤±è´¥ï¼šæœªè½¬æ¢ä»»ä½•æ•°æ®'
                custom_logger.error('âŒ CSV conversion failed: No data converted')
                print(f"CSV conversion failed for task {task_id}")
                return
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            custom_logger.info(f'ğŸ”„ CSV conversion completed: {len(results)} records converted')
            custom_logger.info('ğŸ§  Starting brand analysis...')
            analysis_tasks[task_id]['progress'] = 'CSVè½¬æ¢å®Œæˆï¼Œå¼€å§‹å“ç‰Œåˆ†æ...'
            
            # ä½¿ç”¨è½¬æ¢åçš„JSONæ–‡ä»¶
            file_path = json_file_path
        else:
            custom_logger.info('ğŸ“„ JSON file detected, starting analysis...')
            analysis_tasks[task_id]['progress'] = 'JSONæ–‡ä»¶æ£€æµ‹å®Œæˆï¼Œå¼€å§‹åˆ†æ...'
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹ï¼Œä¼ å…¥è‡ªå®šä¹‰logger
        print(f"Creating analyzer for task {task_id}")
        analyzer = UniversalBrandAnalyzer(output_dir=RESULTS_FOLDER, custom_logger=custom_logger)
        
        # è¿è¡Œåˆ†æ
        print(f"Running analysis for task {task_id}")
        analysis_tasks[task_id]['progress'] = 'æ­£åœ¨è¿›è¡Œå“ç‰Œåˆ†æ...'
        results = analyzer.analyze_creators(file_path)
        
        print(f"Analysis completed for task {task_id}, results count: {len(results) if results else 0}")
        
        if results:
            # ä¿å­˜ç»“æœ
            custom_logger.info('ğŸ’¾ Saving analysis results...')
            analysis_tasks[task_id]['progress'] = 'ä¿å­˜åˆ†æç»“æœ...'
            analyzer.save_results(results, file_path)
            
            # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            brand_file = f"{base_name}_brand_related_{timestamp}.csv"
            non_brand_file = f"{base_name}_non_brand_{timestamp}.csv"
            
            # ç»Ÿè®¡ä¿¡æ¯
            brand_count = sum(1 for r in results if r.is_brand)
            matrix_count = sum(1 for r in results if r.is_matrix_account)
            ugc_count = sum(1 for r in results if r.is_ugc_creator)
            
            brand_related_results = [r for r in results if 
                                   (r.extracted_brand_name and r.extracted_brand_name.strip()) or 
                                   r.is_brand or r.is_matrix_account]
            
            brand_in_related = sum(1 for r in brand_related_results if r.is_brand)
            matrix_in_related = sum(1 for r in brand_related_results if r.is_matrix_account)
            ugc_in_related = sum(1 for r in brand_related_results if r.is_ugc_creator)
            
            # ç¡®ä¿ä»»åŠ¡ä»ç„¶å­˜åœ¨äºå­—å…¸ä¸­ï¼ˆé˜²æ­¢è¢«æ¸…ç†ï¼‰
            if task_id in analysis_tasks:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                analysis_tasks[task_id].update({
                    'status': 'completed',
                    'progress': 'ğŸ‰ åˆ†æå®Œæˆï¼',
                    'completed_at': datetime.now().isoformat(),
                    'results': {
                        'total_count': len(results),
                        'brand_related_count': len(brand_related_results),
                        'non_brand_count': len(results) - len(brand_related_results),
                        'brand_count': brand_count,
                        'matrix_count': matrix_count,
                        'ugc_count': ugc_count,
                        'brand_in_related': brand_in_related,
                        'matrix_in_related': matrix_in_related,
                        'ugc_in_related': ugc_in_related,
                        'brand_file': brand_file,
                        'non_brand_file': non_brand_file
                    }
                })
                custom_logger.info(f'âœ… Analysis completed successfully! Processed {len(results)} creators')
                print(f"Task {task_id} completed successfully")
            else:
                print(f"Warning: Task {task_id} was removed from analysis_tasks during processing")
        else:
            if task_id in analysis_tasks:
                analysis_tasks[task_id]['status'] = 'error'
                analysis_tasks[task_id]['progress'] = 'åˆ†æå¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•åˆ›ä½œè€…'
            custom_logger.error('âŒ Analysis failed: No creators were successfully analyzed')
            print(f"Analysis failed for task {task_id}: No results")
            
    except Exception as e:
        error_msg = f'åˆ†æå¤±è´¥ï¼š{str(e)}'
        print(f"Exception in run_analysis for task {task_id}: {str(e)}")
        
        # ç¡®ä¿ä»»åŠ¡ä»ç„¶å­˜åœ¨
        if task_id in analysis_tasks:
            analysis_tasks[task_id]['status'] = 'error'
            analysis_tasks[task_id]['progress'] = error_msg
            analysis_tasks[task_id]['error_at'] = datetime.now().isoformat()
        
        # ä¹Ÿå°è¯•è®°å½•åˆ°æ—¥å¿—
        try:
            if 'custom_logger' in locals():
                custom_logger.error(f'âŒ Analysis error: {str(e)}')
        except:
            pass
    
    finally:
        # æ¸…ç†è‡ªå®šä¹‰logger
        try:
            if 'custom_logger' in locals():
                custom_logger.handlers = []
        except:
            pass
        
        print(f"Analysis thread for task {task_id} finished")

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({'status': 'healthy', 'message': 'Brand Analyzer API is running'})

@app.route('/api/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file selected'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    if file and allowed_file(file.filename):
        # ç”Ÿæˆå”¯ä¸€çš„ä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        filename = secure_filename(file.filename)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{filename}"
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)
        
        # æ£€æµ‹æ–‡ä»¶ç±»å‹
        is_csv = file.filename.lower().endswith('.csv')
        file_type = 'CSV' if is_csv else 'JSON'
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        analysis_tasks[task_id] = {
            'status': 'pending',
            'progress': f'{file_type} file uploaded successfully, preparing to start analysis...',
            'file_path': file_path,
            'filename': file.filename,
            'file_type': file_type.lower(),
            'logs': [],
            'created_at': datetime.now().isoformat()
        }
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æ
        thread = threading.Thread(target=run_analysis, args=(task_id, file_path, is_csv))
        thread.daemon = True
        thread.start()
        
        return jsonify({'task_id': task_id})
    
    return jsonify({'error': 'Unsupported file format, please upload JSON or CSV file'}), 400

@app.route('/api/status')
def get_status():
    task_id = request.args.get('task_id')
    if not task_id:
        return jsonify({'error': 'Missing task_id parameter'}), 400
    
    print(f"Getting status for task {task_id}")
    print(f"Current tasks in memory: {list(analysis_tasks.keys())}")
    
    if task_id in analysis_tasks:
        task_data = analysis_tasks[task_id]
        print(f"Task {task_id} status: {task_data.get('status', 'unknown')}")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        debug_info = {
            'task_exists': True,
            'current_tasks_count': len(analysis_tasks),
            'task_created_at': task_data.get('created_at', 'unknown')
        }
        
        response_data = {**task_data, 'debug': debug_info}
        return jsonify(response_data)
    
    print(f"Task {task_id} not found in analysis_tasks")
    return jsonify({
        'error': 'Task not found', 
        'debug': {
            'task_exists': False,
            'current_tasks_count': len(analysis_tasks),
            'available_tasks': list(analysis_tasks.keys())[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªä»»åŠ¡ID
        }
    }), 404

@app.route('/api/download')
def download_file():
    task_id = request.args.get('task_id')
    file_type = request.args.get('file_type')
    
    if not task_id or not file_type:
        return jsonify({'error': 'Missing required parameters'}), 400
    
    if task_id not in analysis_tasks:
        return jsonify({'error': 'Task not found'}), 404
    
    task = analysis_tasks[task_id]
    if task['status'] != 'completed':
        return jsonify({'error': 'Analysis not completed yet'}), 400
    
    if file_type == 'brand_related':
        filename = task['results']['brand_file']
    elif file_type == 'non_brand':
        filename = task['results']['non_brand_file']
    else:
        return jsonify({'error': 'Invalid file type'}), 400
    
    file_path = os.path.join(RESULTS_FOLDER, filename)
    if os.path.exists(file_path):
        return send_file(file_path, as_attachment=True, download_name=filename)
    
    return jsonify({'error': 'File not found'}), 404

@app.route('/api/logs')
def get_logs():
    task_id = request.args.get('task_id')
    if not task_id:
        return jsonify({'error': 'Missing task_id parameter'}), 400
    
    print(f"Getting logs for task {task_id}")
    
    if task_id in analysis_tasks:
        logs = analysis_tasks[task_id].get('logs', [])
        print(f"Task {task_id} has {len(logs)} log entries")
        return jsonify({'logs': logs})
    
    print(f"Task {task_id} not found for logs")
    return jsonify({'error': 'Task not found'}), 404

if __name__ == '__main__':
    start_cleanup_thread() # å¯åŠ¨æ¸…ç†çº¿ç¨‹
    app.run(debug=True, host='0.0.0.0', port=5000) 